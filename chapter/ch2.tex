\chapter{相关理论与技术}

\section{概论}

本章对后续章节所用到的相关理论和技术进行简介，主要包括常用的聚类算法（k-means算法和k-medoids算法）以及聚类算法的评估标准、稀疏矩阵的存储方式和多项式拟合技术，对最优化理论进行了简单的阐述，最后简单介绍了马尔科夫链模型。这些理论为后续研究提供了理论机基础。

\section{聚类算法}
聚类算法是一种无监督学习算法，及数据集中的每个数据项不带有标签信息，通过统计分析技术将数据集分成不同的子集或簇，使得分到同一子集或簇的成员对象都有某种相似的特征。这种技术被广泛应用与各个领域，包括机器学习、数据挖掘、模式识别等。

\subsection{k-means 算法}

k-means算法由文献\cite{hartigan1979algorithm}正式提出，现已成为数据挖掘中常用的聚类算法，其算法主要求解思路是一种启发性算法，求解过程利用了EM算法\citing{AMaximum}，通过不断迭代优化直到满足停止迭代的条件。

\subsubsection{算法流程及复杂度分析}

现有样本集$D=\left\{x_1,x_2,...,x_m\}\right\}$，K-Means算法将数据集划分成互不相交的K个簇$C=\left\{C_1,C_2,...,C_K\}\right\}$,且满足$C_1\bigcup{C_2\bigcup{...\bigcup{C_m=D}}}$，每个簇的簇心$\mu _i=\frac{1}{|C_i|}\sum_{\mathbf{x}\in C_i}{\mathbf{x}}$。
其流程如下：\\
\begin{algorithm}[H]
	\label{kmeans}
	 \KwData{样本集$D=\left\{x_1,x_2,...,x_m\}\right\}$，簇个数K}
	 \KwResult{簇划分$C=\left\{C_1,C_2,...,C_k\}\right\}$}
	 从数据集D中随机初始化K个样本，将其作为K个初始簇心\;
	 \Repeat{所有簇心不再更新或更新值小于一定阈值}{
		 将所有的簇置为空，$C_i=\phi \,\,\left( 1\leqslant i\leqslant K \right)$\\
		 \For{j=1,2,...,m}{
		 	计算样本$x_i$与所有簇心之间的距离，并将$x_i$归为第$\alpha _i$类，使其满足：
		 	$\alpha _i=arg\,\,\min \left( d_{i\alpha _i} \right) $\\其中：
		 	$d_{i\alpha _i}=||x_i-\mu _{\alpha _i}||_2$
		 	将样本$x_i$划入簇$C_{\alpha _i}$
		 }
		 \For{i=1,2,...,K}{
		 	$\hat{\mu}_i=\frac{1}{|C_i|}\sum_{x\in C_i}{x}$\\
		 	\If{$\hat{\mu}_i\ne \mu _i$}{
		 		$\mu _i$=$\hat{\mu}_i$
		 	}
		 }
	 }
	 \caption{k-means 算法}
\end{algorithm}

从上述流程可以得到，k-means算法复杂度为O(n*k*t)，其中n表示样本数目，k表示簇个数，t是平均迭代次数。k-meas算法的收敛性可参考论文\cite{neal1998view}。

\subsection{k-medoids 算法}
k-means算法要求元素维度相同且距离度量必须基于欧式距离，如果需要对不规整的数据进行聚类，k-means算法显然无法解决此类聚类问题。针对不规整的数据，可以采用k-medoids算法\citing{rdusseeun1987clustering}进行聚类。k-medoids算法只需输入距离矩阵即可，所以对距离度量方法也更加宽泛。k-medoids流程如下：\\
\begin{algorithm}[H]
	 \KwData{样本距离矩阵$M_{nn}$，簇个数K}
	 \KwResult{簇划分$C=\left\{C_1,C_2,...,C_k\}\right\}$}
	 随机初始化K个样本，将其作为K个初始簇心\;
	 \Repeat{所有簇心不再更新或更新值小于一定阈值}{
		 将所有的簇置为空，$C_i=\phi \,\,\left( 1\leqslant i\leqslant K \right)$\\
		 \For{j=1,2,...,m}{
		 	依据距离矩阵，计算序号为i的样本与所有簇心之间的距离，并将序号为i的样本归为第$\alpha _i$类，使其满足：
		 	$\alpha _i=arg\,\,\min \left( M[i，alpha _i] \right) $\\
		 }
		 \For{i=1,2,...,K}{
		 	$\hat{\mu }_i=arg\,\,\min_{x\in C_i} \left( \sum_{j\in C_i}{M\left[ x,j \right]} \right) $\\
		 	\If{$\hat{\mu}_i\ne \mu _i$}{
		 		$\mu _i$=$\hat{\mu}_i$
		 	}
		 }
	 }
	 \caption{k-medoids 算法}
\end{algorithm}

从上述流程可以得到，k-medoids算法复杂度同k-means一样为O(n*k*t)，其中n表示样本数目，k表示簇个数，t是平均迭代次数。


\subsection{聚类评估指标}
聚类评估指标可以分为两类\citing{zhouzh2016}，
一类是将聚类结果与某个“参考模型”进行比较，度量这种比较结果的评估指标称为“外部指标”；另一类是直接评估聚类效果本身，不参考任何其他模型，这种评估指标称为“内部指标”。

\subsubsection{外部指标}
假设通过聚类算法将数据集D划分成簇集$C=\left\{C_1,C_2,...,C_K\}\right\}$，参考模型将数据集D划分的簇集$C^*=\left\{ C_{1}^{*},C_{2}^{*},...,C_{K}^{*} \right\} $，相应地，令$\lambda$和$\lambda^{*}$分别表示与C和$C^{*}$对应的簇标记。我们将样本两两配对，定义：
\begin{equation}
\label{abcd}
\begin{aligned}
&\left.a=|S S|, \quad S S=\left\{\left(x_{i}, x_{j}\right) | \lambda_{i}=\lambda_{j}, \lambda_{i}^{*}=\lambda_{j}^{*}, i<j\right)\right\}\\
&\left.b=|S D|, \quad S D=\left\{\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right) | \lambda_{i}=\lambda_{j}, \lambda_{i}^{*} \neq \lambda_{j}^{*}, i<j\right)\right\}\\
&\left.c=|D S|, \quad D S=\left\{\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right) | \lambda_{i} \neq \lambda_{j}, \lambda_{i}^{*}=\lambda_{j}^{*}, i<j\right)\right\}\\
&\left.d=|D D|, \quad D D=\left\{\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right) | \lambda_{i} \neq \lambda_{j}, \lambda_{i}^{*} \neq \lambda_{j}^{*}, i<j\right)\right\}
\end{aligned}
\end{equation}

其中集合SS包含了在C中隶属于相同簇且在$C^{*}$中也隶属于相同簇的样本对，集合SD包含了在C中隶属于相同簇但在$C^{*}$中隶属于不同簇的样本对，......由于样本对$\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right)(i<j)$仅能出现在一个集合中，因此有$a+b+c+d=m(m-1) / 2$成立。

基于\ref{abcd}，可引出Rand指数的定义：
\begin{equation}
\label{RI}
\mathrm{RI}=\frac{2(a+d)}{m(m-1)}
\end{equation}

\subsubsection{内部指标}

若直接评估聚类结果本身，则需要考虑内部指标，同样设定聚类算法得到的簇划分为$C=\left\{C_1,C_2,...,C_K\right\}$。在定义内部指标之前，先定义几种距离公式：
\begin{equation}
\label{distance}
\begin{aligned}
avg(C) &=\frac{2}{|C|(|C|-1)} \sum_{1 \leqslant i<j \leqslant|C|} dist\left(x_{i}, x_{j}\right) \\
diam(C) &=\max _{1 \leqslant i<j \leqslant|C|} dist\left(x_{i}, x_{j}\right) \\
d_{\min }\left(C_{i}, C_{j}\right) &=\min _{x_{i} \in C_{i}, x_{j} \in C_{j}} dist\left(x_{i}, x_{j}\right) \\
d_{cen}\left(C_{i}, C_{j}\right) &=dist\left(\mu_{i}, \mu_{j}\right)
\end{aligned}
\end{equation}

其中，dist(.)用于计算两个样本之间的距离；$\mu$代表簇C的中心点。avg(C)对应于簇C内样本间的平均距离，diam(C)对应于簇C内样本间的最远距离，$d_{\min }\left(C_{i}, C_{j}\right)$对应于簇$C_i$与簇$C_j$最近样本间的距离，$d_{cen}\left(C_{i}, C_{j}\right)$对应于簇$C_i$与簇$C_j$中心点间的距离。

基于\ref{distance}可引出以下两种常用内部指标：
\begin{equation}
\label{DBI}
DBI=\frac{1}{k} \sum_{i=1}^{k} \max _{j \neq i}\left(\frac{\operatorname{avg}\left(C_{i}\right)+\operatorname{avg}\left(C_{j}\right)}{d_{\operatorname{con}}\left(\mu_{i}, \mu_{j}\right)}\right)
\end{equation}
\begin{equation}
\label{DI}
DI=\min _{1 \leqslant i \leqslant k}\left\{\min _{j \neq i}\left(\frac{d_{\min }\left(C_{i}, C_{j}\right)}{\max _{1 \leqslant l \leqslant k} \operatorname{diam}\left(C_{l}\right)}\right)\right\}
\end{equation}

DBI的值越小表示聚类效果越好，DI相反，其值越大表示聚类效果更好。

\section{矩阵稀疏存储格式}

计算机经常会对矩阵形式的数据进行处理，在实际场景中处理的矩阵往往包含很多零元素，大部分元素为零元素的矩阵称为稀疏矩阵，对于稀疏矩阵，可以通过特定的存储方式来代替矩阵格式的存储方式，可以大大减少存储的压力。

\subsection{COO存储格式}

COO存储格式对矩阵中的非零元素的坐标和取值进行存储，利用row向量、column向量和value向量来表示稀疏矩阵中的非零元素。假设现在有矩阵T，其内容如下：
\[T=\left[ \begin{matrix}
	0&		0&		1&		0\\
	3&		0&		0&		0\\
	0&		0&		0&		4\\
	2&		0&		0&		0\\
\end{matrix} \right] \]

利用COO存储格式则将矩阵A转换成如下三个向量：
\[\begin{matrix}
	row&		column&		value\\
	0&		2&		1\\
	\begin{array}{c}
	1\\
	\begin{array}{c}
	2\\
	3\\
\end{array}\\
\end{array}&		\begin{array}{c}
	0\\
	\begin{array}{c}
	3\\
	0\\
\end{array}\\
\end{array}&		\begin{array}{c}
	3\\
	\begin{array}{c}
	4\\
	2\\
\end{array}\\
\end{array}\\
\end{matrix}\]

利用坐标和取值来存储稀疏矩阵还有另一种方式：DOK（Dictionary of keys），只不过是通过键值对来表示非零元素的坐标和取值的，上式的矩阵T可以通过如下键值对来表示：
\[\begin{matrix}
	key&		value\\
	\begin{array}{c}
	\left( 0,2 \right)\\
	\begin{array}{c}
	\begin{array}{c}
	\left( 1,0 \right)\\
	\left( 2,3 \right)\\
\end{array}\\
	\left( 3,0 \right)\\
\end{array}\\
\end{array}&		\begin{array}{c}
	\begin{array}{c}
	\begin{array}{c}
	1\\
	3\\
\end{array}\\
	4\\
\end{array}\\
	2\\
\end{array}\\
\end{matrix}\]

\subsection{CSR存储格式}

CRS存储是基于COO存储的改进\citing{bulucc2009parallel}，CRS通过三个向量来存储稀疏矩阵：A,IA和JA。A向量是存储矩阵所有非零元素的数组，元素顺序是行优先，即从左到右、从上往下，如图\ref{rowmajor}所示，IA向量中的元素是递推得到的，IA元素的个数为行数加一，递推关系如下：
\[
\begin{aligned}
IA\left[ 0 \right] &=0
\\
IA\left[ i \right] &=IA\left[ i-1 \right] +第i\text{行非零元素的个数} 0<i<m
\end{aligned}
\]
第三个向量JA是A中每个元素的列索引，同样针对稀疏矩阵T，我们利用CRS存储方式可以表示为：
\[
\begin{aligned}
A&=\left[ 1,3,4,2 \right] 
\\
IA&=\left[ 0,1,2,3,4 \right] 
\\
JA&=\left[ 2,0,3,0 \right] 
\end{aligned}
\]
\begin{figure}[h]
	\includegraphics[width=0.5\textwidth]{rowmajor.png}
	\caption{CRS存储元素的顺序}
	\label{rowmajor}
\end{figure}

\section{多项式拟合}
轨迹数据中的每条轨迹一般是由若干个点组成，有时需要通过这若干个点来拟合轨迹曲线，本文使用多项式来实现轨迹的拟合。
\subsection{理论基础}
在二维空间里，一条轨迹由n个点组成：$\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \dots,\left(x_{k}, y_{k}\right)\right\}$，若轨迹对应的函数为$f(x)$，多项式拟合则试图通过多项式来近似$f(x)$，n-1阶多项式的形式为$g\left( x \right) =a_0+a_1x+\cdots +a_{n-1}x^{n-1}$。

若通过n-1阶多项式来对n个坐标点对应的轨迹进行拟合，则将n个点的坐标带入n-1阶多项式可得到n个等式，将这n个等式通过矩阵表示则为：
\begin{equation}
\label{vandermonde}
\left[ \begin{matrix}
	1&		x_1&		\dots&		x_{1}^{n-1}\\
	1&		x_2&		\dots&		x_{2}^{n-1}\\
	\vdots&		\vdots&		\ddots&		\vdots\\
	1&		x_n&		\dots&		x_{n}^{n-1}\\
\end{matrix} \right] \left[ \begin{array}{c}
	a_0\\
	a_1\\
	\vdots\\
	a_{n-1}\\
\end{array} \right] =\left[ \begin{array}{c}
	y_1\\
	y_2\\
	\vdots\\
	y_n\\
\end{array} \right] 
\end{equation}

记$A=\left[ \begin{matrix}
	1&		x_1&		\dots&		x_{1}^{n-1}\\
	1&		x_2&		\dots&		x_{2}^{n-1}\\
	\vdots&		\vdots&		\ddots&		\vdots\\
	1&		x_n&		\dots&		x_{n}^{n-1}\\
\end{matrix} \right] $
，$\left|A^{T}\right|$则为范德蒙德行列式，故有：
\begin{equation}
\left|A^{T}\right|=\prod_{p>q}\left(x_{p}-x_{q}\right)
\end{equation}

若$x_0,x_1,…,x_n$彼此不相等，故此时存在唯一一个n-1次阶多项式能够包含所有数据点，此时用n-1阶多项式拟合的函数可以经过每一个点。

\subsection{麦克劳林级数及其收敛域}
若函数f(x)在$x_0$的某一个邻域内具有任意阶导数，则幂级数$\sum_{n=0}^{\infty}{\frac{f^{\left( n \right)}\left( x_0 \right)}{n!}}\left( x-x_0 \right) ^n$称为f(x)在$x_0$处的泰勒级数；当$x_0=0$时，此幂级数则称为麦克劳林级数，即$\sum_{n=0}^{\infty}{\frac{f^{\left( n \right)}\left( 0 \right)}{n!}}\left( x \right) ^n$。

假设f(x)在$x_0=0$处的麦克劳林级数收敛于f(x)，即当$n\rightarrow \infty $时，f(x)在$x_0=0$处的麦克劳林公式的余项$R_n(x)$对邻域中的点趋于0。此时，则有：
\begin{equation}
\label{maclaurin}
f\left( x \right) =\sum_{n=0}^{\infty}{\frac{f^{\left( n \right)}\left( 0 \right)}{n!}}\left( x \right) ^n
\end{equation}

等式\ref{maclaurin}右侧是幂级数，关于幂级数的收敛半径满足阿尔贝定理。

\textbf{阿贝尔定理}：如果幂级数$\sum_{n=0}^{\infty}{a_nx^n}$当$x=x_0(x_0 \ne 0)$时收敛，则对于一切满足$x<|x_0|$的x，幂级数$\sum_{n=0}^{\infty}{a_nx^n}$均绝对收敛；如果幂级数$\sum_{n=0}^{\infty}{a_nx^n}$在$x=x_1$处发散，则对于一切$x>|x_1|$的x，幂级数$\sum_{n=0}^{\infty}{a_nx^n}$均发散。

因此我们依据\ref{maclaurin}和阿贝尔定理可以得到函数f(x)其麦克劳林级数的收敛域。

\section{最优化理论}
很多机器学习问题在求解过程中面对的都是在指定约束条件下的最优化问题，其一般形式如下：
\[
minimize\quad f_0\left( x \right) \\
s.t.\quad f_i\left( x \right) \le 0,\quad i=1,...,m
\]

其中$f_0(x)$称为目标函数，$f_{i}(x) \leq 0, \quad i=1, \ldots, m$称为约束函数。若优化问题没有任何约束函数，则称此优化问题为无约束优化问题。

\subsection{经验误差与正则化}
在机器学习中，优化目标一般称为代价函数，代价函数一般与所训练的模型$\hat{f}\left( x \right) $在训练集上的误差相关，学得的模型$\hat{f}\left( x \right) $在训练集上的误差称为经验误差，经验误差是模型$\hat{f}\left( x \right) $关于训练集的平均损失：
\[
R_{\text{emg}}\left( \hat{f} \right) =\frac{1}{N}\sum_{i=1}^N{L}\left( y_i,\hat{f}\left( x_i \right) \right) 
\]

其中，$L\left( y_i,\hat{f}\left( x_i \right) \right) $表示在样本$x_i$上，模型$\hat{f}\left( x_i \right)$来判断输出的损失度量。

如果只追求训练得到的模型$\hat{f}\left( x \right) $使得经验损失尽可能得小，意味着模型$\hat{f}\left( x \right) $在训练集上表现得很好，但是往往会因为模型$\hat{f}\left( x \right) $的复杂度过高而导致对未知数据的预测能力很差，即所谓的泛化能力很差，这种现象称为过拟合，为了防止过拟合现象的出现，人们往往给经验损失添加一个正则项，这种优化经验损失和正则项的方法称为正则化，正则化一般有如下形式：
\begin{equation}
\label{regularization}
\min_{\hat{f}\in \mathcal{F}} \frac{1}{N}\sum_{i=1}^N{L}\left( y_i,\hat{f}\left( x_i \right) \right) +\lambda J\left( \hat{f} \right) 
\end{equation}

其中，第一项是经验风险，第二项是正则项，$\lambda \geqslant 0$为调整两者之间关系的系数。

正则化可以取不同的形式，最常见两种是$L_1$范数和$L_2$范数。$L_1$范数表示为：
\[
L\left( w \right) =\frac{1}{N}\sum_{i=1}^N{\left( \hat{f}\left( x_i;w \right) -y_i \right) ^2}+\lambda |w|_1
\]

这里，$|w|_1$表示参数向量的$L_1$范数。$L_2$范数表示为：
\[L\left( w \right) =\frac{1}{N}\sum_{i=1}^N{\left( \hat{f}\left( x_i;w \right) -y_i \right) ^2}+\frac{\lambda}{2}|w|^2\]

这里，$|w|_2$表示参数向量的$L_2$范数。

正则化符合奥卡姆剃刀原理：在所有可能选择的模型中，能够很好的解释已知数据并且十分简单的才是最好的模型。

\subsection{梯度下降法}
梯度下降法是求解无约束优化问题的一种方法，若函数可微，从函数的任意一点，通过梯度下降法可以找到函数的局部极小值。

梯度下降方法基于以下的观察：如果实值函数F(\textbf{x})在点\textbf{a}处可微且有定义，那么函数F(\textbf{x})在\textbf{a}点沿着梯度相反的方向$-\nabla F(\mathbf{a})$下降最快。因而，如果
\[\mathbf{b}=\mathbf{a}-\gamma \nabla F\left( \mathbf{a} \right) \]
对于$\gamma$ >0为一个够小数值时成立，那么$F(\mathbf{a})\geq F(\mathbf{b})$。

考虑到这一点，我们可以从函数F的局部极小值的初始估计$\mathbf{x}_0$出发，并考虑如下序列$\mathbf{x}_{0}, \mathbf{x}_{1}, \mathbf{x}_{2}, \dots$使得
\[
\mathbf{x}_{n+1}=\mathbf{x}_n-\gamma_n \nabla F(\mathbf{x}_n),\ n \ge 0
\]

因此可得到
\[
F(\mathbf{x}_{0})\geq F(\mathbf{x})_{1})\geq F(\mathbf{x})_{2})\geq \cdots
\]

如果顺利的话序列${\textbf{x}_n}$收敛到期望的局部极小值。注意每次迭代步长$\gamma$ 可以改变。

图\ref{GD}示例了这一过程，这里假设F定义在平面上，并且函数图像是一个碗形。蓝色的曲线是等高线。红色的箭头指向该点梯度的反方向。沿着梯度下降方向，将最终到达碗底，即函数F局部极小值的点。
\begin{figure}[h]
	\includegraphics[width=0.5\textwidth]{GD.png}
	\caption{梯度下降示意图}
	\label{GD}
\end{figure}


\section{马尔科夫链理论}

\subsection{模型简介}

马尔可夫链，又称离散时间马可夫链\citing{norris1998markov}，
是状态空间中从一个状态到另一个状态的转换的随机过程，该过程要求具备“无记忆”的性质：下一状态的概率分布只能由当前状态决定，在时间序列中它前面的事件均与之无关。这种特定类型的“无记忆性”称作马可夫性质。马尔科夫链作为实际过程的统计模型具有许多应用。\\
马尔可夫链中的每一步，可以依据概率分别从一个状态转移到另一状态，与状态之间改变相关的概率叫做转移概率。已知满足马尔可夫性质的随机变量序列X1，X2，X3，...，依据马尔科夫性质的“无记忆性”有：
\begin{equation}
\label{markov_nature}
p\left(X_{n+1}=x | X_{1}=x_{1}, X_{2}=x_{2}, \ldots, X_{n}=x_{n}\right)=p\left(X_{n+1}=x | X_{n}=x_{n}\right)
\end{equation}

当转移概率与n无关时，则称此马尔科夫链是时齐马尔科夫链，本文后续提出的模型是建立在时齐马尔科夫链模型基础上构建的。时齐马尔科夫链满足：
\begin{equation}
\label{time_homogeneity}
p\left(X_{n+1}=x | X_{n}=y\right)=p\left(X_{n}=x | X_{n-1}=y\right)
\end{equation}


\subsubsection{随机游走}
随机游走是一种数学统计模型，它是一连串的轨迹所组成，其中每一次都是随机的。一种较常见的随机游走模型是在规则点阵上进行，称为点阵随机游走。在每一步中，标记的位置根据特定的概率分布从一点跳到另一个点。在简单点阵随机游走中，每一点只能跳到点阵中的相邻位置，形成点阵路径。

设定简单点阵随机游走是时齐的，其转移概率除了满足\ref{time_homogeneity}，还满足：
\begin{equation}
\label{random_walk}
p_{ij}=p\left( X_{n+1}=i|X_n=j \right) =0 \left( i\notin Adj\left( j \right) \right) 
\\
\sum_{i\in Adj\left( j \right)}{p_{ij}=1}
\end{equation}

其中，Adj(j)表示点阵中与点j相邻的点的集合。

\section{本章小结}

本章对本文涉及到的理论技术进行了简要的概述。首先介绍了两种聚类算法：k-means算法和k-medoids算法，并通过伪代码展示了算法的流程，并分析了聚类算法的两种评估指标，即内部指标和外部指标；然后介绍了多项式拟合技术，引出了幂级数中关于收敛域的讨论；随后简单介绍了最优化理论，并针对无约束优化问题引出了经验误差加正则化的通用方法和常用求解算法；最后介绍了马尔科夫链模型，阐述了马尔科夫链建模的特性。本章介绍的所有理论技术对后文研究起到了举足轻重的作用。