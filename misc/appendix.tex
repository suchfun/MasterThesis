
\thesisappendix

\chapter{k-means算法收敛性证明}

k-means算法流程第二章已提到，见\ref{kmeans}，但k-means算法是否收敛从算法力流程中无从得知，这里给出k-means算法收敛的数学证明供参考。

由于k-means算法是由EM算法推导的，故这里首先介绍EM算法。EM算法是针对包含隐变量的问题，令X表示观测变量集，Y表示隐变量集，$\varTheta $表示模型参数变量集，联合分布记为$P(X,Y|\theta)$，条件分布记为$P(Y|X,\theta)$，则EM算法流程可以通过以下步骤表示：
\begin{enumerate}
\item 选择参数的初始值$\theta ^{\left( 0 \right)}$；
\item E步：记$\theta ^{\left( i \right)}$为第i次迭代参数$\theta$参数的估计值，在第i+1次迭代的E步，计算
$$\begin{aligned}
	Q\left( \theta ,\theta ^{\left( i \right)} \right) &=E_Y\left[ \log P\left( X,Y|\theta \right) |X,\theta ^{\left( i \right)} \right]\\
	&=\sum_Y{\log}P\left( X,Y|\theta \right) P\left( Y|X,\theta ^{\left( i \right)} \right)\\
\end{aligned}$$

其中，$P\left( Y|X,\theta ^{\left( i \right)} \right)$是在给定观测数据X和当前的参数估计$\theta ^{\left( i \right)}$下隐变量数据Y的条件概率分布；
\item M步：求使$Q\left( \theta ,\theta ^{\left( i \right)} \right)$极大化的$\theta$，确定第i+1次迭代的参数的估计值$\theta ^{\left( i+1 \right)}$
$$
\theta^{(i+1)}=\arg \max _{\theta} Q\left(\theta, \theta^{(i)}\right)
$$
\item 重复第（2）步和第（3）步直到收敛。 
\end{enumerate}

在k-means算法中，观测变量X则为数据属性，隐变量Y则为数据所属的簇序号，在此基础上定义：
$$P\left( X|y,\theta \right) \propto \exp \left\{ -\left( x-u_y \right) ^2 \right\} $$
$$P\left( Y|x,\theta \right) =\left\{ p_1,p_2,...,p_y,...,p_k \right\} $$

其中$u_y$表示序号为y的簇心，$p_y=1$若x距离$u_y$最近，则$p_i=0,i=1,2,...,y-1,y+1,...,k$。

通过以上定义，经过EM算法的E步，数据集中的所有元素将被归入离自己最近的簇心向量对应的簇，M步针对单个簇中的所有元素，可转换成下式：
$$\begin{aligned}
	\theta ^{\left( i+1 \right)}&=\,\,arg\,\,\underset{\theta}{\max}Q\left( \theta ,\theta ^{\left( i \right)} \right)\\
	&\Leftrightarrow arg\,\,\underset{\theta}{\max}LL\left( X|y,\theta \right)\\
	&\propto arg\,\,\underset{\theta}{\max}\,\,\ln \left( \prod_{i&=1}^n{\exp \left( -|x_i-u_y|^2 \right)} \right)\\
	&\propto arg\,\,\underset{\theta}{\max}\,\,\sum_{i&=1}^n{-|x_i-u_y|^2}\\
\end{aligned}$$

对上式等号右边求导可得：
$$u_y=\frac{\sum_{i=1}^n{x_i}}{n}$$

即，由M步得到的结果是，各个簇心更新为簇中所有元素向量的均值，此结论与k-means算法流程等同。

以上介绍了k-means算法在EM算法层面的解释，要想知道k-means算法的收敛性，只要证明EM算法的收敛性即可，下面则对EM算法收敛性进行说明。关于EM算法收敛性可以由两个定理得到。

\textbf{定理一}： 设$P(X|\theta)$为观测数据的似然函数，$\theta^{(i)}$为EM算法得到的参数估计序列，$P(X|\theta^{(i)})$为桂英的似然函数序列，则$P(X|\theta^{(i)})$是单调递增的，即$P\left( X|\theta ^{\left( i+1 \right)} \right) \geqslant P\left( X|\theta ^{\left( i \right)} \right) $。

\textbf{定理二}： 设$L(\theta)=lnP(X|\theta)$为观测数据的对数似然函数，$\theta^{(i)}$为EM算法得到的参数估计序列，$L(\theta^{(i)})$为对应的对数似然函数序列。则（1）如果$P(X|\theta)$由上界，则$L(\theta^{(i)})=lnP(X|\theta^{(i)}$收敛到某一值L；（2）在函数$Q(\theta,\theta^{(i)})$与$L(\theta)$满足一定条件下，由EM算法得到的参数估计序列$\theta^{(i)}$的收敛值$\theta^{*}$是$L(\theta)$的稳定点。

其中定理二关于函数$Q(\theta,\theta^{(i)})$与$L(\theta)$的条件在大多情况下都是满足的。关于定理的证明可以参考书籍\cite{李航2012统计学习方法}和论文\cite{neal1998view}，这里不再给出证明过程。

有上述两个定理可得，EM算法在大多数情况都能证明过程是收敛的，故k-means算法也有着同样的收敛性。